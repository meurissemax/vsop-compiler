\documentclass[a4paper, 12pt]{article}


%%%%%%%%%%%%
% Packages %
%%%%%%%%%%%%

\usepackage[english]{babel}
\usepackage[noheader]{sleek/sleek}
\usepackage{sleek/sleek-title}


%%%%%%%%%%%%%%%%%%%%%%%
% Title page settings %
%%%%%%%%%%%%%%%%%%%%%%%

\logo{resources/pdf/logo-uliege.pdf}
\institute{University of Li√®ge}
\title{Project : the VSOP compiler}
\subtitle{Compilers}
\author{
    Maxime \textsc{Meurisse} (s161278)\\
    Valentin \textsc{Vermeylen} (s162864)\\
}
\context{Master in Engineering and Computer Science}
\date{Academic year 2019-2020}


%%%%%%%%%%%%
% Document %
%%%%%%%%%%%%

\begin{document}
    % ----- Title page ----- %
    
    \maketitle
    
    % ----- Introduction ----- %
    
    \section{Introduction}
    
    \subsection{Overview}
    
    The goal of this project was to implement a compiler for the VSOP language. To do this, we decided to use the Python language and the \href{https://www.dabeaz.com/ply/}{PLY} library (for lexical and syntactic analysis).
    
    Our compiler is divided into 4 tools : a lexer that performs the lexical analysis, a parser that performs the syntax analysis, a semantics tool that performs the semantic analysis and finally an LLVM tool that generates the LLVM executable corresponding to the VSOP source file.
    
    Each tool works with the previous one. A classical execution of the compiler is as follows : a lexical analysis is first performed. If no errors occurred, a syntax analysis is performed based on the tokens generated by the lexer. If no error occurred, the compiler then performs a semantic analysis based on the AST ({\it Abstract Syntax Tree}) generated by the parser. Finally, if still no error has occurred, the LLVM code and the executable are generated based on the annotated AST generated by the semantic analysis.
    
    The precise operation of each tool as well as the data structures and methods used are explained in the following sections.
    
    \subsection{Error handling}
    
    For error handling, we started from the following general principle : {\it each tool displays as many relevant errors as possible and stops the compiler execution after displaying its errors}.
    
    We chose to display as many relevant errors as possible (and not just one) because we think this system is more convenient for any developer who wants to work with this compiler. Indeed, it seems simpler to fix several potentially independent errors at once rather than having to fix them one by one each time the compiler is run.
    
    Some tools derogate from this rule in certain cases. Details are explained in the sections of each tool.
    
    We have also chosen to stop the compiler execution if a tool returns one (or more) error(s). Indeed, we believe that an error in one tool will almost systematically lead to an error in the next analysis tool (for example, an incorrect token will lead to an error in the syntax analysis since no grammar rule will be found for this token). We made this choice in order to avoid a snowball effect and thus not to display a large number of errors to the user, some of which making no sense.
    
    \subsection{Code organisation}
    
    The archive contains several folders, a \texttt{main.py} file and a \texttt{Makefile}.
    
    Each tool is located in the folder with its name. The \texttt{main.py} file is the main compiler file. The \texttt{Makefile} is used to install all the tools needed by the compiler user (\texttt{make install-tools}) and to generate an executable compiled version of the compiler (\texttt{make vsopc}).
    
    After generating an executable compiled version, the compiler can be used via \texttt{./vsopc <VSOP-SOURCE-FILE>}. Several options are available and can be accessed via \texttt{./vsopc -h}.
    
    % ----- Lexical analysis ----- %
    
    \section{Lexical analysis}
    
    For the lexical analysis, we used the Python PLY library. This provides a \texttt{lex.py} module which is a pure Python implementation of the well-known "lex" tool.
    
    This tool first asks to define a list of tokens. Once this list is defined, a regular expression must be defined for each token (a Python function containing the processing of this regular expression must be created for each token).
    
    When the regular expressions are defined, we just have to launch the tool. It will read the VSOP source file and try to associate each element (each character or group of characters) to a regular expression. The tool uses the {\it longest prefix match} rule to avoid any ambiguity if a token matches several regular expressions.
    
    If an element does not match any regular expression, a special error function is called.
    
    \subsection{String literal}
    
    Tokens "string-literal" were a difficult part of this tool. Indeed, managing special characters, escaped characters and invalid characters was a challenge.
    
    We implemented a special function to handle this type of token. It works correctly except for one very specific case : when a line break is entered incorrectly (not preceded by a \textbackslash) and is not on the first line of the string (because the string would be split on several lines), then the position of this line break is not correctly reported, but the error is still displayed with a meaningful message.
    
    \subsection{Comment handling}
    
    To manage comments, we have implemented several states in our lexer: a "normal" state and a "comment" state.
    
    When the lexer is in its "normal" state, it tries to match tokens with all encoded regular expressions (including comments). If the lexer matches a "comment opening" token, then it switches to the "comment" state.
    
    When the lexer is in the "comment" state, it no longer considers the encoded regular expressions. It ignores all the elements of the file until it encounters either a line break if it is a one-line comment, or an "start/end of comment" token if it is a multi-line comment.
    
    To manage comments on several lines that can be nested, we used a stack. Each time a comment opening ("\texttt{(*}") is encountered, its position is added to the stack. Each time a comment close ("\texttt{*)}") is encountered, an item is removed from the stack. If the stack is empty when an item needs to be removed, or the stack is still full at the end of the VSOP file, an error has occurred.
    
    \subsection{Error handling}
    
    The error handling of this tool respects the previously stated rule : an error is reported for each wrong token in the VSOP file (and all these errors are displayed). If one or more errors occur, the lexer stops the parser execution.
    
    % ----- Syntax analysis ----- %
    
    \section{Syntax analysis}
    
    For the syntax analysis, we used the Python PLY library. This provides a \texttt{yacc.py} module which is a pure Python implementation of the well-known "yacc" tool. This tool is a {\it Look Ahead Left-to-Right (LALR) parser generator} and works in a bottom-up way.
    
    This tool requires to define a series of grammar rules. These rules may involve other rules or tokens. Once these rules are defined, the tool can be launched to parse a VSOP file.
    
    It works as follows : the parser takes as argument the lexer at its instantiation. It then uses the lexer to generate the tokens one by one. As the tokens are generated, the parser tries to match a grammar rule. If no grammar rule could be matched, a special error function is called.
    
    For each grammar rule matched, the corresponding Python function is called with the different elements of the grammar rule as arguments. Each function called creates a new node in the AST.
    
    \subsection{Abstract Syntax Tree}
    
    We have created a customized structure to generate the AST. For this, we used the object-oriented paradigm : each node of the tree is an object and contains references to its child nodes. The root node of the tree is an object called "Program". It is this object that will symbolize the AST.
    
    For example, when the AST will have been generated, the "Program" object will contain a reference to all the "Class" objects representing the different classes composing the VSOP program. Each "Class" object will contain references to "Field" and "Method" objects representing respectively the fields and methods defined in the class. Each "Method" object will contain references to "Formal" objects, etc.
    
    We have also taken advantage of the inheritance mechanism : each node inherits from a "Node" class and each expression inherits from an "Expr" class (which itself inherits from the "Node" class).
    
    When a grammar rule is matched, we instantiate a new object representing that rule and return it. Each rule returns the instantiated object (except the root node rule), so the AST is built recursively as we parsed.
    
    \subsection{Precedence rules}
    
    The grammar defining the VSOP language is ambiguous. To remove this ambiguity, precedence rules have been defined.
    
    The PLY "lex" tool allows us to encode these rules simply in a tuple. Within this tuple declaration, tokens are ordered from lowest to highest precedence.
    
    \subsection{Error handling}
    
    Handling errors in the parser is a real challenge. By default, when no grammar rule could be matched, an error function is called. This function displays an error in \texttt{stderr}. However, his message is not very clear; it is simply a "syntax error" message with the faulty token.
    
    We have therefore tried to implement clearer and more personalized error messages. For this, we have encoded false grammar rules. When one of these rules is matched, we can then display a personalized error message according to the matched rule.
    
    For example, if a semicolon is required, we encode the grammar rule with the semicolon but also the one without the semicolon. The first rule will be tested and not matched. The second will then be tested and matched. We will then know that we are in the case of a forgotten semicolon and we will be able to display a corresponding error message.
    
    {\bf To complete depending of what we try (single or all error messages).}
    
    % ----- Semantic analysis ----- %
    
    \section{Semantic analysis}
    
    For the semantic analysis, we did not use any tool or library; we implemented all the analysis ourselves.
    
    For this, we used symbol tables (implemented using the object-oriented paradigm) and an Eulerian traversal of the AST.
    
    \subsection{Passes in AST}
    
    Our semantic analysis tool takes the AST as an argument when it is instantiated and returns this same AST where all the expressions have been annotated of their type.
    
    For this, we run through the AST several times in order to make all the necessary checks and annotations.
    
    First, we partially scan the AST to check the class declarations (we iterate on the list of classes in the "Program" object). For each class encountered, we check if it has not already been declared and we associate a table of symbols with it. This table contains information such as the name of the class and its position as well as references to the symbol table of its parent class (either a custom class or the "Object" class), the symbol tables of its fields and the symbol tables of its methods. At the end of this first pass in the tree, we are sure that all the class declarations are valid (no re-definition or cycle for example).
    
    Second, we continue with two partial passes : a first to check the declarations of fields and a second to check the declarations of methods. These passes are very similar to that concerning the classes : checks are made (to be sure that a field has not been re-defined or that a method is re-defined correctly, etc.) and a symbol table is created for every element. The symbol tables created are linked to the corresponding class.
    
    Finally, we make a last pass in the tree to check the type of each expression. This pass in the tree is complete and is in the form of an Eulerian route. The analysis is done recursively : when an expression is analyzed, all the expressions making it up are first analyzed. So we analyze the deepest expressions in the AST first. If an expression is valid, it is annotated with its type, otherwise an error message is displayed.
    
    \subsection{Symbol tables}
    
    We used the object-oriented paradigm to implement the symbol tables. We have created a symbol table for each element that can define a new scope, namely classes, methods and "let". We also created a similar object for the fields.
    
    Each table contains references to the tables in its scope. For example, the symbol table of a class will contain references to the symbol tables of its fields and methods.
    
    To check if an element is well defined in a scope, we used a stack in our expression analysis : when the tool walks in the tree, it adds and removes the symbol tables from the stack according to the elements that he crosses. For example, when working in a class, the symbol table for that class will be added to the stack. If a "let" is defined in this class, its symbol table will be added to the stack. Thus, when checking a scope, the most recent element on the stack will be analyzed first. In this example, the tool will look first in the symbol table of the "let" and then in the symbol table of the class.
    
    \subsection{Error handling}
    
    In this tool, it is easier to display clear and personalized error messages because all the analysis is done by us. In addition, this consists of analyzing each type of expression with well-defined functions; it is therefore simple to know in what context the error was raised.
    
    {\bf To complete depending of what we try (single or all error messages).}
    
    % ----- Code generation ----- %
    
    \section{Code generation}
    
    to do
    
    % ----- Limitations and retrospective analysis ----- %
    
    \section{Limitations and retrospective analysis}
    
    We are quite satisfied with the result obtained for our compiler. This seems to work in the vast majority of cases.
    
    {\bf To complete.}
    
    % ----- Conclusion ----- %
    
    \section{Conclusion}
    
    This project allowed us to discover and learn a lot of new things. We were able to discover the main principles of a compiler and deal with the different difficulties of implementing such a tool.
    
    The VSOP language for which we had to create a compiler was well defined, complete and affordable enough to create a first compiler.
    
    We spent a large amount of hours on this project, while staying within the reasonable limits defined by the course workload.
    
    {\bf To complete.}
\end{document}
